{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "narrow-claim",
   "metadata": {},
   "outputs": [],
   "source": [
    "from json_utils import read_json, read_jsonl\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "import re\n",
    "\n",
    "# stop words\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as en_stop_words\n",
    "from spacy.lang.da.stop_words import STOP_WORDS as da_stop_words\n",
    "nltk_da_stop_words = stopwords.words(\"danish\")\n",
    "nltk_en_stop_words = stopwords.words(\"english\")\n",
    "from string import punctuation\n",
    "stop_words = list(en_stop_words) + list(da_stop_words) + nltk_da_stop_words + nltk_en_stop_words + ['#dkpol'] + list(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "smoking-flour",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94, 179, 326, 219)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk_da_stop_words), len(nltk_en_stop_words), len(en_stop_words), len(da_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "super-category",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ../data/dkpol_tweets.jsonl\n"
     ]
    }
   ],
   "source": [
    "data = read_jsonl(\"../data/dkpol_tweets.jsonl\")\n",
    "text = [tweet[\"text\"] for tweet in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "standard-paintball",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    clean_tokens = \" \".join([token.lower().strip() for token in tokens if token.lower().strip() not in stop_words])\n",
    "    clean_tokens = clean_tokens.strip().split(\" \")\n",
    "    return clean_tokens\n",
    "        \n",
    "\n",
    "def clean_doc(doc):\n",
    "    doc = re.sub(r'http\\S+', \"\", doc.strip()) #remove links\n",
    "    doc = re.sub(r'^[A-Za-z]', \"\", doc.strip()) #remove special chars\n",
    "    doc = ''.join([ch for ch in doc if ord(ch) < 250]) #remove special characters with a hex code > 250(æ:230, ø:248, å:229)\n",
    "    return doc\n",
    "\n",
    "\n",
    "def tokenize_docs(docs):\n",
    "    tt = TweetTokenizer()\n",
    "    tokens = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        doc = clean_doc(doc)\n",
    "        if doc:\n",
    "            doc = tt.tokenize(doc)\n",
    "            if doc:\n",
    "                doc = remove_stopwords(doc)\n",
    "                if len(doc) > 2:\n",
    "                    tokens.append(doc)\n",
    "                \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "exact-monroe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts(tokenized_docs):\n",
    "    counter = Counter()\n",
    "    for doc in tokenized_docs:\n",
    "        counter.update(doc)\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eleven-fleet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_counts(counts):\n",
    "    return sorted(counts.items(), key=lambda k: k[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "exposed-preview",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize_docs(text)\n",
    "counts = get_counts(tokens)\n",
    "sorted_counts = sort_counts(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-geneva",
   "metadata": {},
   "source": [
    "# Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "incoming-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_n_words(counts, n):\n",
    "    top_n = counts[:n]\n",
    "    word, counts = zip(*top_n)\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 8))\n",
    "    y_pos = np.arange(n)\n",
    "    plt.bar(y_pos, counts, align='center', alpha=0.5, width=0.9)\n",
    "    plt.xticks(y_pos, word,rotation=75)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Top {n} tokens in tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-language",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_n_words(sorted_counts, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-laptop",
   "metadata": {},
   "source": [
    "# Word Frequency Per Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-graphics",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-lesbian",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.created_at = pd.to_datetime(df.created_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-ordinary",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"month\"] = df.created_at.dt.to_period(\"M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-leave",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_10_tokens_per_month(df):\n",
    "    months = df[\"month\"].unique()\n",
    "    n_months = len(months)\n",
    "    #fig, axs = plt.subplots(math.floor(n_months/2), math.ceil(n_months/2), figsize=(15,15))\n",
    "    \n",
    "    for month in months:\n",
    "        df1 = df[df[\"month\"]==month]\n",
    "        text = [text for text in df1.text]\n",
    "        tokens = tokenize_docs(text)\n",
    "        counts = get_counts(tokens)\n",
    "        sorted_counts = sort_counts(counts)\n",
    "        plot_top_n_words(sorted_counts, 10)\n",
    "        \n",
    "top_10_tokens_per_month(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-douglas",
   "metadata": {},
   "source": [
    "# TF-IDF "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-motion",
   "metadata": {},
   "source": [
    "    Todo: Create better tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "widespread-cycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer=\"word\", tokenizer=dummy_fun, preprocessor=dummy_fun)\n",
    "X = vectorizer.fit_transform(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-operation",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = dict(zip(X.indices[np.argsort(X.data)], X.data[np.argsort(X.data)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-italic",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in vectorizer.vocabulary_.items():\n",
    "    dic[key] = dic[val]\n",
    "    del dic[val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-color",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_word_counts = sort_counts(dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "textile-liberal",
   "metadata": {},
   "source": [
    "#### Clustering of the tfidf embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "czech-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "advisory-combine",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-national",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_embeddings = umap.UMAP(n_neighbors=20,\n",
    "                            min_dist=0.1,\n",
    "                            n_components=2, \n",
    "                            metric='cosine').fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-texture",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = hdbscan.HDBSCAN(min_cluster_size=20,\n",
    "                          metric='euclidean',                      \n",
    "                          cluster_selection_method='eom').fit(umap_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-defendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(umap_embeddings, columns=['x', 'y'])\n",
    "result['labels'] = cluster.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-circle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "outliers = result.loc[result.labels == -1, :]\n",
    "clustered = result.loc[result.labels != -1, :]\n",
    "#plt.scatter(outliers.x, outliers.y, color='#BDBDBD', s=0.05)\n",
    "plt.scatter(clustered.x, clustered.y, c=clustered.labels, s=0.05, cmap='hsv_r')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-angel",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(x=clustered.x, y=clustered.y, color=clustered.labels, color_discrete_map='hsv_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-motion",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = result.loc[result.loc[:, \"labels\"]==178].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-settle",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(feature_names[indexes]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-airfare",
   "metadata": {},
   "source": [
    "# Clustering with k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-secret",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-choir",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_clusters(data, max_k):\n",
    "    iters = range(2, max_k+1, 2)\n",
    "    \n",
    "    sse = []\n",
    "    for k in iters:\n",
    "        sse.append(MiniBatchKMeans(n_clusters=k, init_size=1024, batch_size=2048, random_state=20).fit(data).inertia_)\n",
    "        print('Fit {} clusters'.format(k))\n",
    "        \n",
    "    f, ax = plt.subplots(1, 1)\n",
    "    ax.plot(iters, sse, marker='o')\n",
    "    ax.set_xlabel('Cluster Centers')\n",
    "    ax.set_xticks(iters)\n",
    "    ax.set_xticklabels(iters)\n",
    "    ax.set_ylabel('SSE')\n",
    "    ax.set_title('SSE by Cluster Center Plot')\n",
    "    \n",
    "find_optimal_clusters(umap_embeddings, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-democrat",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = MiniBatchKMeans(n_clusters=40, init_size=1024, batch_size=2048, random_state=20).fit_predict(umap_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-index",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-coaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(umap_embeddings, columns=['x', 'y'])\n",
    "result['labels'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-apple",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "outliers = result.loc[result.labels == -1, :]\n",
    "clustered = result.loc[result.labels != -1, :]\n",
    "#plt.scatter(outliers.x, outliers.y, color='#BDBDBD', s=0.05)\n",
    "plt.scatter(clustered.x, clustered.y, c=clustered.labels, s=0.05, cmap='hsv_r')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-truck",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = result.loc[result.loc[:, \"labels\"]==1].index.values\n",
    "print(list(feature_names[indexes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-roads",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_keywords(data, clusters, labels, n_terms):\n",
    "    df = pd.DataFrame(data.todense()).groupby(clusters).mean()\n",
    "    print(df)\n",
    "    \n",
    "    for i,r in df.iterrows():\n",
    "        print('\\nCluster {}'.format(i))\n",
    "        print(','.join([labels[t] for t in np.argsort(r)[-n_terms:]]))\n",
    "            \n",
    "get_top_keywords(X[:100], clusters[:100], vectorizer.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-bibliography",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-relay",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-terrorism",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X.todense()[:1000]).groupby(clusters[:1000]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-generic",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flow",
   "language": "python",
   "name": "flow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
